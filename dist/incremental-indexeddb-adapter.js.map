{
  "version": 3,
  "sources": ["../src/incremental-indexeddb-adapter.ts"],
  "sourcesContent": ["// @ts-nocheck\n/* jshint -W030 */\nconst DEBUG =\n  typeof window !== \"undefined\" && !!window.__loki_incremental_idb_debug;\n\n/**\n * An improved Loki persistence adapter for IndexedDB (not compatible with LokiIndexedAdapter)\n *     Unlike LokiIndexedAdapter, the database is saved not as one big JSON blob, but split into\n *     small chunks with individual collection documents. When saving, only the chunks with changed\n *     documents (and database metadata) is saved to IndexedDB. This speeds up small incremental\n *     saves by an order of magnitude on large (tens of thousands of records) databases. It also\n *     avoids Safari 13 bug that would cause the database to balloon in size to gigabytes\n *\n *     The `appname` argument is not provided - to distinguish between multiple app on the same\n *     domain, simply use a different Loki database name\n *\n * @example\n * var adapter = new IncrementalIndexedDBAdapter();\n *\n * @constructor IncrementalIndexedDBAdapter\n *\n * @param {object=} options Configuration options for the adapter\n * @param {function} options.onversionchange Function to call on `IDBDatabase.onversionchange` event\n *     (most likely database deleted from another browser tab)\n * @param {function} options.onFetchStart Function to call once IDB load has begun.\n *     Use this as an opportunity to execute code concurrently while IDB does work on a separate thread\n * @param {function} options.onDidOverwrite Called when this adapter is forced to overwrite contents\n *     of IndexedDB. This happens if there's another open tab of the same app that's making changes.\n *     You might use it as an opportunity to alert user to the potential loss of data\n * @param {function} options.serializeChunk Called with a chunk (array of Loki documents) before\n *     it's saved to IndexedDB. You can use it to manually compress on-disk representation\n *     for faster database loads. Hint: Hand-written conversion of objects to arrays is very\n *     profitable for performance. If you use this, you must also pass options.deserializeChunk.\n * @param {function} options.deserializeChunk Called with a chunk serialized with options.serializeChunk\n *     Expects an array of Loki documents as the return value\n * @param {number} options.megachunkCount Number of parallel requests for data when loading database.\n *     Can be tuned for a specific application\n * @param {array} options.lazyCollections Names of collections that should be deserialized lazily\n *     Only use this for collections that aren't used at launch\n */\nclass IncrementalIndexedDBAdapter {\n  constructor(options) {\n    this.mode = \"incremental\";\n    this.options = options || {};\n    this.chunkSize = 100;\n    this.megachunkCount = this.options.megachunkCount || 24;\n    this.lazyCollections = this.options.lazyCollections || [];\n    this.idb = null; // will be lazily loaded on first operation that needs it\n    this._prevLokiVersionId = null;\n    this._prevCollectionVersionIds = {};\n\n    if (!(this.megachunkCount >= 4 && this.megachunkCount % 2 === 0)) {\n      throw new Error(\"megachunkCount must be >=4 and divisible by 2\");\n    }\n  }\n\n  // chunkId - index of the data chunk - e.g. chunk 0 will be lokiIds 0-99\n  _getChunk(collection, chunkId) {\n    // 0-99, 100-199, etc.\n    const minId = chunkId * this.chunkSize;\n    const maxId = minId + this.chunkSize - 1;\n\n    // use idIndex to find first collection.data position within the $loki range\n    collection.ensureId();\n    const idIndex = collection.idIndex;\n\n    let firstDataPosition = null;\n\n    let max = idIndex.length - 1;\n    let min = 0;\n    let mid;\n\n    while (idIndex[min] < idIndex[max]) {\n      mid = (min + max) >> 1;\n\n      if (idIndex[mid] < minId) {\n        min = mid + 1;\n      } else {\n        max = mid;\n      }\n    }\n\n    if (max === min && idIndex[min] >= minId && idIndex[min] <= maxId) {\n      firstDataPosition = min;\n    }\n\n    if (firstDataPosition === null) {\n      // no elements in this chunk\n      return [];\n    }\n\n    // find last position\n    // if loki IDs are contiguous (no removed elements), last position will be first + chunk - 1\n    // (and we look back in case there are missing pieces)\n    // TODO: Binary search (not as important as first position, worst case scanario is only chunkSize steps)\n    let lastDataPosition = null;\n    for (\n      let i = firstDataPosition + this.chunkSize - 1;\n      i >= firstDataPosition;\n      i--\n    ) {\n      if (idIndex[i] <= maxId) {\n        lastDataPosition = i;\n        break;\n      }\n    }\n\n    // verify\n    const firstElement = collection.data[firstDataPosition];\n    if (\n      !(\n        firstElement &&\n        firstElement.$loki >= minId &&\n        firstElement.$loki <= maxId\n      )\n    ) {\n      throw new Error(\"broken invariant firstelement\");\n    }\n\n    const lastElement = collection.data[lastDataPosition];\n    if (\n      !(lastElement && lastElement.$loki >= minId && lastElement.$loki <= maxId)\n    ) {\n      throw new Error(\"broken invariant lastElement\");\n    }\n\n    // this will have *up to* 'this.chunkSize' elements (might have less, because $loki ids\n    // will have holes when data is deleted)\n    const chunkData = collection.data.slice(\n      firstDataPosition,\n      lastDataPosition + 1\n    );\n\n    if (chunkData.length > this.chunkSize) {\n      throw new Error(\"broken invariant - chunk size\");\n    }\n\n    return chunkData;\n  }\n\n  /**\n   * Incrementally saves the database to IndexedDB\n   *\n   * @example\n   * var idbAdapter = new IncrementalIndexedDBAdapter();\n   * var db = new loki('test', { adapter: idbAdapter });\n   * var coll = db.addCollection('testColl');\n   * coll.insert({test: 'val'});\n   * db.saveDatabase();\n   *\n   * @param {string} dbname - the name to give the serialized database\n   * @param {function} getLokiCopy - returns copy of the Loki database\n   * @param {function} callback - (Optional) callback passed obj.success with true or false\n   * @memberof IncrementalIndexedDBAdapter\n   */\n  saveDatabase(dbname, getLokiCopy, callback) {\n    const that = this;\n\n    if (!this.idb) {\n      this._initializeIDB(dbname, callback, () => {\n        that.saveDatabase(dbname, getLokiCopy, callback);\n      });\n      return;\n    }\n\n    if (this.operationInProgress) {\n      throw new Error(\n        \"Error while saving to database - another operation is already in progress. Please use throttledSaves=true option on Loki object\"\n      );\n    }\n    this.operationInProgress = true;\n\n    DEBUG && console.log(\"saveDatabase - begin\");\n    DEBUG && console.time(\"saveDatabase\");\n    function finish(e) {\n      DEBUG && e && console.error(e);\n      DEBUG && console.timeEnd(\"saveDatabase\");\n      that.operationInProgress = false;\n      callback(e);\n    }\n\n    // try..catch is required, e.g.:\n    // InvalidStateError: Failed to execute 'transaction' on 'IDBDatabase': The database connection is closing.\n    // (this may happen if another tab has called deleteDatabase)\n    try {\n      let updatePrevVersionIds = () => {\n        console.error(\n          \"Unexpected successful tx - cannot update previous version ids\"\n        );\n      };\n      let didOverwrite = false;\n\n      const tx = this.idb.transaction([\"LokiIncrementalData\"], \"readwrite\");\n      tx.oncomplete = () => {\n        updatePrevVersionIds();\n        finish();\n        if (didOverwrite && that.options.onDidOverwrite) {\n          that.options.onDidOverwrite();\n        }\n      };\n\n      tx.onerror = (e) => {\n        finish(e);\n      };\n\n      tx.onabort = (e) => {\n        finish(e);\n      };\n\n      const store = tx.objectStore(\"LokiIncrementalData\");\n\n      const performSave = (maxChunkIds) => {\n        try {\n          const incremental = !maxChunkIds;\n          const chunkInfo = that._putInChunks(\n            store,\n            getLokiCopy(),\n            incremental,\n            maxChunkIds\n          );\n          // Update last seen version IDs, but only after the transaction is successful\n          updatePrevVersionIds = () => {\n            that._prevLokiVersionId = chunkInfo.lokiVersionId;\n            chunkInfo.collectionVersionIds.forEach(({ name, versionId }) => {\n              that._prevCollectionVersionIds[name] = versionId;\n            });\n          };\n          tx.commit && tx.commit();\n        } catch (error) {\n          console.error(\"idb performSave failed: \", error);\n          tx.abort();\n        }\n      };\n\n      // Incrementally saving changed chunks breaks down if there is more than one writer to IDB\n      // (multiple tabs of the same web app), leading to data corruption. To fix that, we save all\n      // metadata chunks (loki + collections) with a unique ID on each save and remember it. Before\n      // the subsequent save, we read loki from IDB to check if its version ID changed. If not, we're\n      // guaranteed that persisted DB is consistent with our diff. Otherwise, we fall back to the slow\n      // path and overwrite *all* database chunks with our version. Both reading and writing must\n      // happen in the same IDB transaction for this to work.\n      // TODO: We can optimize the slow path by fetching collection metadata chunks and comparing their\n      // version IDs with those last seen by us. Since any change in collection data requires a metadata\n      // chunk save, we're guaranteed that if the IDs match, we don't need to overwrite chukns of this collection\n      const getAllKeysThenSave = () => {\n        // NOTE: We must fetch all keys to protect against a case where another tab has wrote more\n        // chunks whan we did -- if so, we must delete them.\n        idbReq(\n          store.getAllKeys(),\n          ({ target }) => {\n            const maxChunkIds = getMaxChunkIds(target.result);\n            performSave(maxChunkIds);\n          },\n          (e) => {\n            console.error(\"Getting all keys failed: \", e);\n            tx.abort();\n          }\n        );\n      };\n\n      const getLokiThenSave = () => {\n        idbReq(\n          store.get(\"loki\"),\n          ({ target }) => {\n            if (lokiChunkVersionId(target.result) === that._prevLokiVersionId) {\n              performSave();\n            } else {\n              DEBUG &&\n                console.warn(\n                  \"Another writer changed Loki IDB, using slow path...\"\n                );\n              didOverwrite = true;\n              getAllKeysThenSave();\n            }\n          },\n          (e) => {\n            console.error(\"Getting loki chunk failed: \", e);\n            tx.abort();\n          }\n        );\n      };\n\n      getLokiThenSave();\n    } catch (error) {\n      finish(error);\n    }\n  }\n\n  _putInChunks(idbStore, loki, incremental, maxChunkIds) {\n    const that = this;\n    const collectionVersionIds = [];\n    let savedSize = 0;\n\n    const prepareCollection = (collection, i) => {\n      // Find dirty chunk ids\n      const dirtyChunks = new Set();\n      incremental &&\n        collection.dirtyIds.forEach((lokiId) => {\n          const chunkId = (lokiId / that.chunkSize) | 0;\n          dirtyChunks.add(chunkId);\n        });\n      collection.dirtyIds = [];\n\n      // Serialize chunks to save\n      const prepareChunk = (chunkId) => {\n        let chunkData = that._getChunk(collection, chunkId);\n        if (that.options.serializeChunk) {\n          chunkData = that.options.serializeChunk(collection.name, chunkData);\n        }\n        // we must stringify now, because IDB is asynchronous, and underlying objects are mutable\n        // In general, it's also faster to stringify, because we need serialization anyway, and\n        // JSON.stringify is much better optimized than IDB's structured clone\n        chunkData = JSON.stringify(chunkData);\n        savedSize += chunkData.length;\n        DEBUG &&\n          incremental &&\n          console.log(`Saving: ${collection.name}.chunk.${chunkId}`);\n        idbStore.put({\n          key: `${collection.name}.chunk.${chunkId}`,\n          value: chunkData,\n        });\n      };\n      if (incremental) {\n        dirtyChunks.forEach(prepareChunk);\n      } else {\n        // add all chunks\n        const maxChunkId = (collection.maxId / that.chunkSize) | 0;\n        for (let j = 0; j <= maxChunkId; j += 1) {\n          prepareChunk(j);\n        }\n\n        // delete chunks with larger ids than what we have\n        // NOTE: we don't have to delete metadata chunks as they will be absent from loki anyway\n        // NOTE: failures are silently ignored, so we don't have to worry about holes\n        const persistedMaxChunkId = maxChunkIds[collection.name] || 0;\n        for (let k = maxChunkId + 1; k <= persistedMaxChunkId; k += 1) {\n          const deletedChunkName = `${collection.name}.chunk.${k}`;\n          idbStore.delete(deletedChunkName);\n          DEBUG && console.warn(`Deleted chunk: ${deletedChunkName}`);\n        }\n      }\n\n      // save collection metadata as separate chunk (but only if changed)\n      if (collection.dirty || dirtyChunks.size || !incremental) {\n        collection.idIndex = []; // this is recreated lazily\n        collection.data = [];\n        collection.idbVersionId = randomVersionId();\n        collectionVersionIds.push({\n          name: collection.name,\n          versionId: collection.idbVersionId,\n        });\n\n        const metadataChunk = JSON.stringify(collection);\n        savedSize += metadataChunk.length;\n        DEBUG &&\n          incremental &&\n          console.log(`Saving: ${collection.name}.metadata`);\n        idbStore.put({\n          key: `${collection.name}.metadata`,\n          value: metadataChunk,\n        });\n      }\n\n      // leave only names in the loki chunk\n      loki.collections[i] = { name: collection.name };\n    };\n    loki.collections.forEach(prepareCollection);\n\n    loki.idbVersionId = randomVersionId();\n    const serializedMetadata = JSON.stringify(loki);\n    savedSize += serializedMetadata.length;\n\n    DEBUG && incremental && console.log(\"Saving: loki\");\n    idbStore.put({ key: \"loki\", value: serializedMetadata });\n\n    DEBUG && console.log(`saved size: ${savedSize}`);\n    return {\n      lokiVersionId: loki.idbVersionId,\n      collectionVersionIds,\n    };\n  }\n\n  /**\n   * Retrieves a serialized db string from the catalog.\n   *\n   * @example\n   * // LOAD\n   * var idbAdapter = new IncrementalIndexedDBAdapter();\n   * var db = new loki('test', { adapter: idbAdapter });\n   * db.loadDatabase(function(result) {\n   *   console.log('done');\n   * });\n   *\n   * @param {string} dbname - the name of the database to retrieve.\n   * @param {function} callback - callback should accept string param containing serialized db string.\n   * @memberof IncrementalIndexedDBAdapter\n   */\n  loadDatabase(dbname, callback) {\n    const that = this;\n\n    if (this.operationInProgress) {\n      throw new Error(\n        \"Error while loading database - another operation is already in progress. Please use throttledSaves=true option on Loki object\"\n      );\n    }\n\n    this.operationInProgress = true;\n\n    DEBUG && console.log(\"loadDatabase - begin\");\n    DEBUG && console.time(\"loadDatabase\");\n\n    const finish = (value) => {\n      DEBUG && console.timeEnd(\"loadDatabase\");\n      that.operationInProgress = false;\n      callback(value);\n    };\n\n    this._getAllChunks(dbname, (chunks) => {\n      try {\n        if (!Array.isArray(chunks)) {\n          throw chunks; // we have an error\n        }\n\n        if (!chunks.length) {\n          return finish(null);\n        }\n\n        DEBUG && console.log(\"Found chunks:\", chunks.length);\n\n        // repack chunks into a map\n        chunks = chunksToMap(chunks);\n        const loki = chunks.loki;\n        chunks.loki = null; // gc\n\n        // populate collections with data\n        populateLoki(\n          loki,\n          chunks.chunkMap,\n          that.options.deserializeChunk,\n          that.lazyCollections\n        );\n        chunks = null; // gc\n\n        // remember previous version IDs\n        that._prevLokiVersionId = loki.idbVersionId || null;\n        that._prevCollectionVersionIds = {};\n        loki.collections.forEach(({ name, idbVersionId }) => {\n          that._prevCollectionVersionIds[name] = idbVersionId || null;\n        });\n\n        return finish(loki);\n      } catch (error) {\n        that._prevLokiVersionId = null;\n        that._prevCollectionVersionIds = {};\n        return finish(error);\n      }\n    });\n  }\n\n  _initializeIDB(dbname, onError, onSuccess) {\n    const that = this;\n    DEBUG && console.log(\"initializing idb\");\n\n    if (this.idbInitInProgress) {\n      throw new Error(\n        \"Cannot open IndexedDB because open is already in progress\"\n      );\n    }\n    this.idbInitInProgress = true;\n\n    const openRequest = indexedDB.open(dbname, 1);\n\n    openRequest.onupgradeneeded = ({ target, oldVersion }) => {\n      const db = target.result;\n      DEBUG && console.log(`onupgradeneeded, old version: ${oldVersion}`);\n\n      if (oldVersion < 1) {\n        // Version 1 - Initial - Create database\n        db.createObjectStore(\"LokiIncrementalData\", { keyPath: \"key\" });\n      } else {\n        // Unknown version\n        throw new Error(\n          `Invalid old version ${oldVersion} for IndexedDB upgrade`\n        );\n      }\n    };\n\n    openRequest.onsuccess = ({ target }) => {\n      that.idbInitInProgress = false;\n      const db = target.result;\n      that.idb = db;\n\n      if (!db.objectStoreNames.contains(\"LokiIncrementalData\")) {\n        onError(new Error(\"Missing LokiIncrementalData\"));\n        // Attempt to recover (after reload) by deleting database, since it's damaged anyway\n        that.deleteDatabase(dbname);\n        return;\n      }\n\n      DEBUG && console.log(\"init success\");\n\n      db.onversionchange = (versionChangeEvent) => {\n        // Ignore if database was deleted and recreated in the meantime\n        if (that.idb !== db) {\n          return;\n        }\n\n        DEBUG && console.log(\"IDB version change\", versionChangeEvent);\n        // This function will be called if another connection changed DB version\n        // (Most likely database was deleted from another browser tab, unless there's a new version\n        // of this adapter, or someone makes a connection to IDB outside of this adapter)\n        // We must close the database to avoid blocking concurrent deletes.\n        // The database will be unusable after this. Be sure to supply `onversionchange` option\n        // to force logout\n        that.idb.close();\n        that.idb = null;\n        if (that.options.onversionchange) {\n          that.options.onversionchange(versionChangeEvent);\n        }\n      };\n\n      onSuccess();\n    };\n\n    openRequest.onblocked = (e) => {\n      console.error(\"IndexedDB open is blocked\", e);\n      onError(new Error(\"IndexedDB open is blocked by open connection\"));\n    };\n\n    openRequest.onerror = (e) => {\n      that.idbInitInProgress = false;\n      console.error(\"IndexedDB open error\", e);\n      onError(e);\n    };\n  }\n\n  _getAllChunks(dbname, callback) {\n    const that = this;\n    if (!this.idb) {\n      this._initializeIDB(dbname, callback, () => {\n        that._getAllChunks(dbname, callback);\n      });\n      return;\n    }\n\n    const tx = this.idb.transaction([\"LokiIncrementalData\"], \"readonly\");\n    const store = tx.objectStore(\"LokiIncrementalData\");\n\n    const deserializeChunk = this.options.deserializeChunk;\n    const lazyCollections = this.lazyCollections;\n\n    // If there are a lot of chunks (>100), don't request them all in one go, but in multiple\n    // \"megachunks\" (chunks of chunks). This improves concurrency, as main thread is already busy\n    // while IDB process is still fetching data. Details: https://github.com/techfort/LokiJS/pull/874\n    function getMegachunks(keys) {\n      const megachunkCount = that.megachunkCount;\n      const keyRanges = createKeyRanges(keys, megachunkCount);\n\n      const allChunks = [];\n      let megachunksReceived = 0;\n\n      function processMegachunk({ target }, megachunkIndex, keyRange) {\n        // var debugMsg = 'processing chunk ' + megachunkIndex + ' (' + keyRange.lower + ' -- ' + keyRange.upper + ')'\n        // DEBUG && console.time(debugMsg);\n        const megachunk = target.result;\n        megachunk.forEach((chunk, i) => {\n          parseChunk(chunk, deserializeChunk, lazyCollections);\n          allChunks.push(chunk);\n          megachunk[i] = null; // gc\n        });\n        // DEBUG && console.timeEnd(debugMsg);\n\n        megachunksReceived += 1;\n        if (megachunksReceived === megachunkCount) {\n          callback(allChunks);\n        }\n      }\n\n      // Stagger megachunk requests - first one half, then request the second when first one comes\n      // back. This further improves concurrency.\n      const megachunkWaves = 2;\n      const megachunksPerWave = megachunkCount / megachunkWaves;\n      function requestMegachunk(index, wave) {\n        const keyRange = keyRanges[index];\n        idbReq(\n          store.getAll(keyRange),\n          (e) => {\n            if (wave < megachunkWaves) {\n              requestMegachunk(index + megachunksPerWave, wave + 1);\n            }\n\n            processMegachunk(e, index, keyRange);\n          },\n          (e) => {\n            callback(e);\n          }\n        );\n      }\n\n      for (let i = 0; i < megachunksPerWave; i += 1) {\n        requestMegachunk(i, 1);\n      }\n    }\n\n    function getAllChunks() {\n      idbReq(\n        store.getAll(),\n        ({ target }) => {\n          const allChunks = target.result;\n          allChunks.forEach((chunk) => {\n            parseChunk(chunk, deserializeChunk, lazyCollections);\n          });\n          callback(allChunks);\n        },\n        (e) => {\n          callback(e);\n        }\n      );\n    }\n\n    function getAllKeys() {\n      function onDidGetKeys(keys) {\n        keys.sort();\n        if (keys.length > 100) {\n          getMegachunks(keys);\n        } else {\n          getAllChunks();\n        }\n      }\n\n      idbReq(\n        store.getAllKeys(),\n        ({ target }) => {\n          onDidGetKeys(target.result);\n        },\n        (e) => {\n          callback(e);\n        }\n      );\n\n      if (that.options.onFetchStart) {\n        that.options.onFetchStart();\n      }\n    }\n\n    getAllKeys();\n  }\n\n  /**\n   * Deletes a database from IndexedDB\n   *\n   * @example\n   * // DELETE DATABASE\n   * // delete 'finance'/'test' value from catalog\n   * idbAdapter.deleteDatabase('test', function {\n   *   // database deleted\n   * });\n   *\n   * @param {string} dbname - the name of the database to delete from IDB\n   * @param {function=} callback - (Optional) executed on database delete\n   * @memberof IncrementalIndexedDBAdapter\n   */\n  deleteDatabase(dbname, callback) {\n    if (this.operationInProgress) {\n      throw new Error(\n        \"Error while deleting database - another operation is already in progress. Please use throttledSaves=true option on Loki object\"\n      );\n    }\n\n    this.operationInProgress = true;\n\n    const that = this;\n    DEBUG && console.log(\"deleteDatabase - begin\");\n    DEBUG && console.time(\"deleteDatabase\");\n\n    this._prevLokiVersionId = null;\n    this._prevCollectionVersionIds = {};\n\n    if (this.idb) {\n      this.idb.close();\n      this.idb = null;\n    }\n\n    const request = indexedDB.deleteDatabase(dbname);\n\n    request.onsuccess = () => {\n      that.operationInProgress = false;\n      DEBUG && console.timeEnd(\"deleteDatabase\");\n      callback({ success: true });\n    };\n\n    request.onerror = (e) => {\n      that.operationInProgress = false;\n      console.error(\"Error while deleting database\", e);\n      callback({ success: false });\n    };\n\n    request.onblocked = (e) => {\n      // We can't call callback with failure status, because this will be called even if we\n      // succeed in just a moment\n      console.error(\n        \"Deleting database failed because it's blocked by another connection\",\n        e\n      );\n    };\n  }\n}\n\n// gets current largest chunk ID for each collection\nfunction getMaxChunkIds(allKeys) {\n  const maxChunkIds = {};\n\n  allKeys.forEach((key) => {\n    const keySegments = key.split(\".\");\n    // table.chunk.2317\n    if (keySegments.length === 3 && keySegments[1] === \"chunk\") {\n      const collection = keySegments[0];\n      const chunkId = parseInt(keySegments[2]) || 0;\n      const currentMax = maxChunkIds[collection];\n\n      if (!currentMax || chunkId > currentMax) {\n        maxChunkIds[collection] = chunkId;\n      }\n    }\n  });\n  return maxChunkIds;\n}\n\nfunction lokiChunkVersionId(chunk) {\n  try {\n    if (chunk) {\n      const loki = JSON.parse(chunk.value);\n      return loki.idbVersionId || null;\n    } else {\n      return null;\n    }\n  } catch (e) {\n    console.error(\"Error while parsing loki chunk\", e);\n    return null;\n  }\n}\n\nfunction chunksToMap(chunks) {\n  let loki;\n  const chunkMap = {};\n\n  sortChunksInPlace(chunks);\n\n  chunks.forEach((chunk) => {\n    const type = chunk.type;\n    const value = chunk.value;\n    const name = chunk.collectionName;\n    if (type === \"loki\") {\n      loki = value;\n    } else if (type === \"data\") {\n      if (chunkMap[name]) {\n        chunkMap[name].dataChunks.push(value);\n      } else {\n        chunkMap[name] = {\n          metadata: null,\n          dataChunks: [value],\n        };\n      }\n    } else if (type === \"metadata\") {\n      if (chunkMap[name]) {\n        chunkMap[name].metadata = value;\n      } else {\n        chunkMap[name] = { metadata: value, dataChunks: [] };\n      }\n    } else {\n      throw new Error(\"unreachable\");\n    }\n  });\n\n  if (!loki) {\n    throw new Error(\"Corrupted database - missing database metadata\");\n  }\n\n  return { loki, chunkMap };\n}\n\nfunction populateLoki(\n  { collections },\n  chunkMap,\n  deserializeChunk,\n  lazyCollections\n) {\n  collections.forEach(function populateCollection(collectionStub, i) {\n    const name = collectionStub.name;\n    const chunkCollection = chunkMap[name];\n    if (chunkCollection) {\n      if (!chunkCollection.metadata) {\n        throw new Error(\n          `Corrupted database - missing metadata chunk for ${name}`\n        );\n      }\n      const collection = chunkCollection.metadata;\n      chunkCollection.metadata = null;\n      collections[i] = collection;\n\n      const isLazy = lazyCollections.includes(name);\n      const lokiDeserializeCollectionChunks = () => {\n        DEBUG && isLazy && console.log(`lazy loading ${name}`);\n        const data = [];\n        const dataChunks = chunkCollection.dataChunks;\n        dataChunks.forEach(function populateChunk(chunk, i) {\n          if (isLazy) {\n            chunk = JSON.parse(chunk);\n            if (deserializeChunk) {\n              chunk = deserializeChunk(name, chunk);\n            }\n          }\n          chunk.forEach((doc) => {\n            data.push(doc);\n          });\n          dataChunks[i] = null;\n        });\n        return data;\n      };\n      collection.getData = lokiDeserializeCollectionChunks;\n    }\n  });\n}\n\nfunction classifyChunk(chunk) {\n  const key = chunk.key;\n\n  if (key === \"loki\") {\n    chunk.type = \"loki\";\n    return;\n  } else if (key.includes(\".\")) {\n    const keySegments = key.split(\".\");\n    if (keySegments.length === 3 && keySegments[1] === \"chunk\") {\n      chunk.type = \"data\";\n      chunk.collectionName = keySegments[0];\n      chunk.index = parseInt(keySegments[2], 10);\n      return;\n    } else if (keySegments.length === 2 && keySegments[1] === \"metadata\") {\n      chunk.type = \"metadata\";\n      chunk.collectionName = keySegments[0];\n      return;\n    }\n  }\n\n  console.error(`Unknown chunk ${key}`);\n  throw new Error(\"Corrupted database - unknown chunk found\");\n}\n\nfunction parseChunk(chunk, deserializeChunk, lazyCollections) {\n  classifyChunk(chunk);\n\n  const isData = chunk.type === \"data\";\n  const isLazy = lazyCollections.includes(chunk.collectionName);\n\n  if (!(isData && isLazy)) {\n    chunk.value = JSON.parse(chunk.value);\n  }\n  if (deserializeChunk && isData && !isLazy) {\n    chunk.value = deserializeChunk(chunk.collectionName, chunk.value);\n  }\n}\n\nfunction randomVersionId() {\n  // Appears to have enough entropy for chunk version IDs\n  // (Only has to be different than enough of its own previous versions that there's no writer\n  // that thinks a new version is the same as an earlier one, not globally unique)\n  return Math.random().toString(36).substring(2);\n}\n\nfunction sortChunksInPlace(chunks) {\n  // sort chunks in place to load data in the right order (ascending loki ids)\n  // on both Safari and Chrome, we'll get chunks in order like this: 0, 1, 10, 100...\n  chunks.sort(function (a, b) {\n    return (a.index || 0) - (b.index || 0);\n  });\n}\n\nfunction createKeyRanges(keys, count) {\n  const countPerRange = Math.floor(keys.length / count);\n  const keyRanges = [];\n  let minKey;\n  let maxKey;\n  for (let i = 0; i < count; i += 1) {\n    minKey = keys[countPerRange * i];\n    maxKey = keys[countPerRange * (i + 1)];\n    if (i === 0) {\n      // ... < maxKey\n      keyRanges.push(IDBKeyRange.upperBound(maxKey, true));\n    } else if (i === count - 1) {\n      // >= minKey\n      keyRanges.push(IDBKeyRange.lowerBound(minKey));\n    } else {\n      // >= minKey && < maxKey\n      keyRanges.push(IDBKeyRange.bound(minKey, maxKey, false, true));\n    }\n  }\n  return keyRanges;\n}\n\nfunction idbReq(request, onsuccess, onerror) {\n  request.onsuccess = (e) => {\n    try {\n      return onsuccess(e);\n    } catch (error) {\n      onerror(error);\n    }\n  };\n  request.onerror = onerror;\n  return request;\n}\n\nif (window !== undefined) {\n  window.IncrementalIndexedDBAdapter = IncrementalIndexedDBAdapter;\n}\n"],
  "mappings": "qFAEA,IAAMA,EACJ,OAAO,QAAW,aAAe,CAAC,CAAC,OAAO,6BAqCtCC,EAAN,KAAkC,CAChC,YAAYC,EAAS,CAUnB,GATA,KAAK,KAAO,cACZ,KAAK,QAAUA,GAAW,CAAC,EAC3B,KAAK,UAAY,IACjB,KAAK,eAAiB,KAAK,QAAQ,gBAAkB,GACrD,KAAK,gBAAkB,KAAK,QAAQ,iBAAmB,CAAC,EACxD,KAAK,IAAM,KACX,KAAK,mBAAqB,KAC1B,KAAK,0BAA4B,CAAC,EAE9B,EAAE,KAAK,gBAAkB,GAAK,KAAK,eAAiB,IAAM,GAC5D,MAAM,IAAI,MAAM,+CAA+C,CAEnE,CAGA,UAAUC,EAAYC,EAAS,CAE7B,IAAMC,EAAQD,EAAU,KAAK,UACvBE,EAAQD,EAAQ,KAAK,UAAY,EAGvCF,EAAW,SAAS,EACpB,IAAMI,EAAUJ,EAAW,QAEvBK,EAAoB,KAEpBC,EAAMF,EAAQ,OAAS,EACvBG,EAAM,EACNC,EAEJ,KAAOJ,EAAQG,CAAG,EAAIH,EAAQE,CAAG,GAC/BE,EAAOD,EAAMD,GAAQ,EAEjBF,EAAQI,CAAG,EAAIN,EACjBK,EAAMC,EAAM,EAEZF,EAAME,EAQV,GAJIF,IAAQC,GAAOH,EAAQG,CAAG,GAAKL,GAASE,EAAQG,CAAG,GAAKJ,IAC1DE,EAAoBE,GAGlBF,IAAsB,KAExB,MAAO,CAAC,EAOV,IAAII,EAAmB,KACvB,QACMC,EAAIL,EAAoB,KAAK,UAAY,EAC7CK,GAAKL,EACLK,IAEA,GAAIN,EAAQM,CAAC,GAAKP,EAAO,CACvBM,EAAmBC,EACnB,MAKJ,IAAMC,EAAeX,EAAW,KAAKK,CAAiB,EACtD,GACE,EACEM,GACAA,EAAa,OAAST,GACtBS,EAAa,OAASR,GAGxB,MAAM,IAAI,MAAM,+BAA+B,EAGjD,IAAMS,EAAcZ,EAAW,KAAKS,CAAgB,EACpD,GACE,EAAEG,GAAeA,EAAY,OAASV,GAASU,EAAY,OAAST,GAEpE,MAAM,IAAI,MAAM,8BAA8B,EAKhD,IAAMU,EAAYb,EAAW,KAAK,MAChCK,EACAI,EAAmB,CACrB,EAEA,GAAII,EAAU,OAAS,KAAK,UAC1B,MAAM,IAAI,MAAM,+BAA+B,EAGjD,OAAOA,CACT,CAiBA,aAAaC,EAAQC,EAAaC,EAAU,CAC1C,IAAMC,EAAO,KAEb,GAAI,CAAC,KAAK,IAAK,CACb,KAAK,eAAeH,EAAQE,EAAU,IAAM,CAC1CC,EAAK,aAAaH,EAAQC,EAAaC,CAAQ,CACjD,CAAC,EACD,OAGF,GAAI,KAAK,oBACP,MAAM,IAAI,MACR,iIACF,EAEF,KAAK,oBAAsB,GAE3BnB,GAAS,QAAQ,IAAI,sBAAsB,EAC3CA,GAAS,QAAQ,KAAK,cAAc,EACpC,SAASqB,EAAOC,EAAG,CACjBtB,GAASsB,GAAK,QAAQ,MAAMA,CAAC,EAC7BtB,GAAS,QAAQ,QAAQ,cAAc,EACvCoB,EAAK,oBAAsB,GAC3BD,EAASG,CAAC,CACZ,CALSC,EAAAF,EAAA,UAUT,GAAI,CACF,IAAIG,EAAuBD,EAAA,IAAM,CAC/B,QAAQ,MACN,+DACF,CACF,EAJ2B,wBAKvBE,EAAe,GAEbC,EAAK,KAAK,IAAI,YAAY,CAAC,qBAAqB,EAAG,WAAW,EACpEA,EAAG,WAAa,IAAM,CACpBF,EAAqB,EACrBH,EAAO,EACHI,GAAgBL,EAAK,QAAQ,gBAC/BA,EAAK,QAAQ,eAAe,CAEhC,EAEAM,EAAG,QAAWJ,GAAM,CAClBD,EAAOC,CAAC,CACV,EAEAI,EAAG,QAAWJ,GAAM,CAClBD,EAAOC,CAAC,CACV,EAEA,IAAMK,EAAQD,EAAG,YAAY,qBAAqB,EAE5CE,EAAcL,EAACM,GAAgB,CACnC,GAAI,CACF,IAAMC,EAAc,CAACD,EACfE,EAAYX,EAAK,aACrBO,EACAT,EAAY,EACZY,EACAD,CACF,EAEAL,EAAuBD,EAAA,IAAM,CAC3BH,EAAK,mBAAqBW,EAAU,cACpCA,EAAU,qBAAqB,QAAQ,CAAC,CAAE,KAAAC,EAAM,UAAAC,CAAU,IAAM,CAC9Db,EAAK,0BAA0BY,CAAI,EAAIC,CACzC,CAAC,CACH,EALuB,wBAMvBP,EAAG,QAAUA,EAAG,OAAO,CACzB,OAASQ,EAAP,CACA,QAAQ,MAAM,2BAA4BA,CAAK,EAC/CR,EAAG,MAAM,CACX,CACF,EArBoB,eAiCdS,EAAqBZ,EAAA,IAAM,CAG/Ba,EACET,EAAM,WAAW,EACjB,CAAC,CAAE,OAAAU,CAAO,IAAM,CACd,IAAMR,EAAcS,EAAeD,EAAO,MAAM,EAChDT,EAAYC,CAAW,CACzB,EACCP,GAAM,CACL,QAAQ,MAAM,4BAA6BA,CAAC,EAC5CI,EAAG,MAAM,CACX,CACF,CACF,EAd2B,sBAgBHH,EAAA,IAAM,CAC5Ba,EACET,EAAM,IAAI,MAAM,EAChB,CAAC,CAAE,OAAAU,CAAO,IAAM,CACVE,EAAmBF,EAAO,MAAM,IAAMjB,EAAK,mBAC7CQ,EAAY,GAEZ5B,GACE,QAAQ,KACN,qDACF,EACFyB,EAAe,GACfU,EAAmB,EAEvB,EACCb,GAAM,CACL,QAAQ,MAAM,8BAA+BA,CAAC,EAC9CI,EAAG,MAAM,CACX,CACF,CACF,EApBwB,mBAsBR,CAClB,OAASQ,EAAP,CACAb,EAAOa,CAAK,CACd,CACF,CAEA,aAAaM,EAAUC,EAAMX,EAAaD,EAAa,CACrD,IAAMT,EAAO,KACPsB,EAAuB,CAAC,EAC1BC,EAAY,EAEVC,EAAoBrB,EAAA,CAACpB,EAAYU,IAAM,CAE3C,IAAMgC,EAAc,IAAI,IACxBf,GACE3B,EAAW,SAAS,QAAS2C,GAAW,CACtC,IAAM1C,EAAW0C,EAAS1B,EAAK,UAAa,EAC5CyB,EAAY,IAAIzC,CAAO,CACzB,CAAC,EACHD,EAAW,SAAW,CAAC,EAGvB,IAAM4C,EAAexB,EAACnB,GAAY,CAChC,IAAIY,EAAYI,EAAK,UAAUjB,EAAYC,CAAO,EAC9CgB,EAAK,QAAQ,iBACfJ,EAAYI,EAAK,QAAQ,eAAejB,EAAW,KAAMa,CAAS,GAKpEA,EAAY,KAAK,UAAUA,CAAS,EACpC2B,GAAa3B,EAAU,OACvBhB,GACE8B,GACA,QAAQ,IAAI,WAAW3B,EAAW,cAAcC,GAAS,EAC3DoC,EAAS,IAAI,CACX,IAAK,GAAGrC,EAAW,cAAcC,IACjC,MAAOY,CACT,CAAC,CACH,EAjBqB,gBAkBrB,GAAIc,EACFe,EAAY,QAAQE,CAAY,MAC3B,CAEL,IAAMC,EAAc7C,EAAW,MAAQiB,EAAK,UAAa,EACzD,QAAS6B,EAAI,EAAGA,GAAKD,EAAYC,GAAK,EACpCF,EAAaE,CAAC,EAMhB,IAAMC,EAAsBrB,EAAY1B,EAAW,IAAI,GAAK,EAC5D,QAASgD,EAAIH,EAAa,EAAGG,GAAKD,EAAqBC,GAAK,EAAG,CAC7D,IAAMC,EAAmB,GAAGjD,EAAW,cAAcgD,IACrDX,EAAS,OAAOY,CAAgB,EAChCpD,GAAS,QAAQ,KAAK,kBAAkBoD,GAAkB,GAK9D,GAAIjD,EAAW,OAAS0C,EAAY,MAAQ,CAACf,EAAa,CACxD3B,EAAW,QAAU,CAAC,EACtBA,EAAW,KAAO,CAAC,EACnBA,EAAW,aAAekD,EAAgB,EAC1CX,EAAqB,KAAK,CACxB,KAAMvC,EAAW,KACjB,UAAWA,EAAW,YACxB,CAAC,EAED,IAAMmD,EAAgB,KAAK,UAAUnD,CAAU,EAC/CwC,GAAaW,EAAc,OAC3BtD,GACE8B,GACA,QAAQ,IAAI,WAAW3B,EAAW,eAAe,EACnDqC,EAAS,IAAI,CACX,IAAK,GAAGrC,EAAW,gBACnB,MAAOmD,CACT,CAAC,EAIHb,EAAK,YAAY5B,CAAC,EAAI,CAAE,KAAMV,EAAW,IAAK,CAChD,EAxE0B,qBAyE1BsC,EAAK,YAAY,QAAQG,CAAiB,EAE1CH,EAAK,aAAeY,EAAgB,EACpC,IAAME,EAAqB,KAAK,UAAUd,CAAI,EAC9C,OAAAE,GAAaY,EAAmB,OAEhCvD,GAAS8B,GAAe,QAAQ,IAAI,cAAc,EAClDU,EAAS,IAAI,CAAE,IAAK,OAAQ,MAAOe,CAAmB,CAAC,EAEvDvD,GAAS,QAAQ,IAAI,eAAe2C,GAAW,EACxC,CACL,cAAeF,EAAK,aACpB,qBAAAC,CACF,CACF,CAiBA,aAAazB,EAAQE,EAAU,CAC7B,IAAMC,EAAO,KAEb,GAAI,KAAK,oBACP,MAAM,IAAI,MACR,+HACF,EAGF,KAAK,oBAAsB,GAE3BpB,GAAS,QAAQ,IAAI,sBAAsB,EAC3CA,GAAS,QAAQ,KAAK,cAAc,EAEpC,IAAMqB,EAASE,EAACiC,GAAU,CACxBxD,GAAS,QAAQ,QAAQ,cAAc,EACvCoB,EAAK,oBAAsB,GAC3BD,EAASqC,CAAK,CAChB,EAJe,UAMf,KAAK,cAAcvC,EAASwC,GAAW,CACrC,GAAI,CACF,GAAI,CAAC,MAAM,QAAQA,CAAM,EACvB,MAAMA,EAGR,GAAI,CAACA,EAAO,OACV,OAAOpC,EAAO,IAAI,EAGpBrB,GAAS,QAAQ,IAAI,gBAAiByD,EAAO,MAAM,EAGnDA,EAASC,EAAYD,CAAM,EAC3B,IAAMhB,EAAOgB,EAAO,KACpB,OAAAA,EAAO,KAAO,KAGdE,EACElB,EACAgB,EAAO,SACPrC,EAAK,QAAQ,iBACbA,EAAK,eACP,EACAqC,EAAS,KAGTrC,EAAK,mBAAqBqB,EAAK,cAAgB,KAC/CrB,EAAK,0BAA4B,CAAC,EAClCqB,EAAK,YAAY,QAAQ,CAAC,CAAE,KAAAT,EAAM,aAAA4B,CAAa,IAAM,CACnDxC,EAAK,0BAA0BY,CAAI,EAAI4B,GAAgB,IACzD,CAAC,EAEMvC,EAAOoB,CAAI,CACpB,OAASP,EAAP,CACA,OAAAd,EAAK,mBAAqB,KAC1BA,EAAK,0BAA4B,CAAC,EAC3BC,EAAOa,CAAK,CACrB,CACF,CAAC,CACH,CAEA,eAAejB,EAAQ4C,EAASC,EAAW,CACzC,IAAM1C,EAAO,KAGb,GAFApB,GAAS,QAAQ,IAAI,kBAAkB,EAEnC,KAAK,kBACP,MAAM,IAAI,MACR,2DACF,EAEF,KAAK,kBAAoB,GAEzB,IAAM+D,EAAc,UAAU,KAAK9C,EAAQ,CAAC,EAE5C8C,EAAY,gBAAkB,CAAC,CAAE,OAAA1B,EAAQ,WAAA2B,CAAW,IAAM,CACxD,IAAMC,EAAK5B,EAAO,OAGlB,GAFArC,GAAS,QAAQ,IAAI,iCAAiCgE,GAAY,EAE9DA,EAAa,EAEfC,EAAG,kBAAkB,sBAAuB,CAAE,QAAS,KAAM,CAAC,MAG9D,OAAM,IAAI,MACR,uBAAuBD,yBACzB,CAEJ,EAEAD,EAAY,UAAY,CAAC,CAAE,OAAA1B,CAAO,IAAM,CACtCjB,EAAK,kBAAoB,GACzB,IAAM6C,EAAK5B,EAAO,OAGlB,GAFAjB,EAAK,IAAM6C,EAEP,CAACA,EAAG,iBAAiB,SAAS,qBAAqB,EAAG,CACxDJ,EAAQ,IAAI,MAAM,6BAA6B,CAAC,EAEhDzC,EAAK,eAAeH,CAAM,EAC1B,OAGFjB,GAAS,QAAQ,IAAI,cAAc,EAEnCiE,EAAG,gBAAmBC,GAAuB,CAEvC9C,EAAK,MAAQ6C,IAIjBjE,GAAS,QAAQ,IAAI,qBAAsBkE,CAAkB,EAO7D9C,EAAK,IAAI,MAAM,EACfA,EAAK,IAAM,KACPA,EAAK,QAAQ,iBACfA,EAAK,QAAQ,gBAAgB8C,CAAkB,EAEnD,EAEAJ,EAAU,CACZ,EAEAC,EAAY,UAAazC,GAAM,CAC7B,QAAQ,MAAM,4BAA6BA,CAAC,EAC5CuC,EAAQ,IAAI,MAAM,8CAA8C,CAAC,CACnE,EAEAE,EAAY,QAAWzC,GAAM,CAC3BF,EAAK,kBAAoB,GACzB,QAAQ,MAAM,uBAAwBE,CAAC,EACvCuC,EAAQvC,CAAC,CACX,CACF,CAEA,cAAcL,EAAQE,EAAU,CAC9B,IAAMC,EAAO,KACb,GAAI,CAAC,KAAK,IAAK,CACb,KAAK,eAAeH,EAAQE,EAAU,IAAM,CAC1CC,EAAK,cAAcH,EAAQE,CAAQ,CACrC,CAAC,EACD,OAIF,IAAMQ,EADK,KAAK,IAAI,YAAY,CAAC,qBAAqB,EAAG,UAAU,EAClD,YAAY,qBAAqB,EAE5CwC,EAAmB,KAAK,QAAQ,iBAChCC,EAAkB,KAAK,gBAK7B,SAASC,EAAcC,EAAM,CAC3B,IAAMC,EAAiBnD,EAAK,eACtBoD,EAAYC,EAAgBH,EAAMC,CAAc,EAEhDG,EAAY,CAAC,EACfC,EAAqB,EAEzB,SAASC,EAAiB,CAAE,OAAAvC,CAAO,EAAGwC,EAAgBC,EAAU,CAG9D,IAAMC,EAAY1C,EAAO,OACzB0C,EAAU,QAAQ,CAACC,EAAOnE,IAAM,CAC9BoE,EAAWD,EAAOb,EAAkBC,CAAe,EACnDM,EAAU,KAAKM,CAAK,EACpBD,EAAUlE,CAAC,EAAI,IACjB,CAAC,EAGD8D,GAAsB,EAClBA,IAAuBJ,GACzBpD,EAASuD,CAAS,CAEtB,CAfSnD,EAAAqD,EAAA,oBAmBT,IAAMM,EAAiB,EACjBC,EAAoBZ,EAAiBW,EAC3C,SAASE,EAAiBC,EAAOC,EAAM,CACrC,IAAMR,EAAWN,EAAUa,CAAK,EAChCjD,EACET,EAAM,OAAOmD,CAAQ,EACpBxD,GAAM,CACDgE,EAAOJ,GACTE,EAAiBC,EAAQF,EAAmBG,EAAO,CAAC,EAGtDV,EAAiBtD,EAAG+D,EAAOP,CAAQ,CACrC,EACCxD,GAAM,CACLH,EAASG,CAAC,CACZ,CACF,CACF,CAfSC,EAAA6D,EAAA,oBAiBT,QAASvE,EAAI,EAAGA,EAAIsE,EAAmBtE,GAAK,EAC1CuE,EAAiBvE,EAAG,CAAC,CAEzB,CAhDSU,EAAA8C,EAAA,iBAkDT,SAASkB,GAAe,CACtBnD,EACET,EAAM,OAAO,EACb,CAAC,CAAE,OAAAU,CAAO,IAAM,CACd,IAAMqC,EAAYrC,EAAO,OACzBqC,EAAU,QAASM,GAAU,CAC3BC,EAAWD,EAAOb,EAAkBC,CAAe,CACrD,CAAC,EACDjD,EAASuD,CAAS,CACpB,EACCpD,GAAM,CACLH,EAASG,CAAC,CACZ,CACF,CACF,CAdSC,EAAAgE,EAAA,gBAgBT,SAASC,GAAa,CACpB,SAASC,EAAanB,EAAM,CAC1BA,EAAK,KAAK,EACNA,EAAK,OAAS,IAChBD,EAAcC,CAAI,EAElBiB,EAAa,CAEjB,CAPShE,EAAAkE,EAAA,gBASTrD,EACET,EAAM,WAAW,EACjB,CAAC,CAAE,OAAAU,CAAO,IAAM,CACdoD,EAAapD,EAAO,MAAM,CAC5B,EACCf,GAAM,CACLH,EAASG,CAAC,CACZ,CACF,EAEIF,EAAK,QAAQ,cACfA,EAAK,QAAQ,aAAa,CAE9B,CAvBSG,EAAAiE,EAAA,cAyBTA,EAAW,CACb,CAgBA,eAAevE,EAAQE,EAAU,CAC/B,GAAI,KAAK,oBACP,MAAM,IAAI,MACR,gIACF,EAGF,KAAK,oBAAsB,GAE3B,IAAMC,EAAO,KACbpB,GAAS,QAAQ,IAAI,wBAAwB,EAC7CA,GAAS,QAAQ,KAAK,gBAAgB,EAEtC,KAAK,mBAAqB,KAC1B,KAAK,0BAA4B,CAAC,EAE9B,KAAK,MACP,KAAK,IAAI,MAAM,EACf,KAAK,IAAM,MAGb,IAAM0F,EAAU,UAAU,eAAezE,CAAM,EAE/CyE,EAAQ,UAAY,IAAM,CACxBtE,EAAK,oBAAsB,GAC3BpB,GAAS,QAAQ,QAAQ,gBAAgB,EACzCmB,EAAS,CAAE,QAAS,EAAK,CAAC,CAC5B,EAEAuE,EAAQ,QAAW,GAAM,CACvBtE,EAAK,oBAAsB,GAC3B,QAAQ,MAAM,gCAAiC,CAAC,EAChDD,EAAS,CAAE,QAAS,EAAM,CAAC,CAC7B,EAEAuE,EAAQ,UAAa,GAAM,CAGzB,QAAQ,MACN,sEACA,CACF,CACF,CACF,CACF,EA1pBMnE,EAAAtB,EAAA,+BA6pBN,SAASqC,EAAeqD,EAAS,CAC/B,IAAM9D,EAAc,CAAC,EAErB,OAAA8D,EAAQ,QAASC,GAAQ,CACvB,IAAMC,EAAcD,EAAI,MAAM,GAAG,EAEjC,GAAIC,EAAY,SAAW,GAAKA,EAAY,CAAC,IAAM,QAAS,CAC1D,IAAM1F,EAAa0F,EAAY,CAAC,EAC1BzF,EAAU,SAASyF,EAAY,CAAC,CAAC,GAAK,EACtCC,EAAajE,EAAY1B,CAAU,GAErC,CAAC2F,GAAc1F,EAAU0F,KAC3BjE,EAAY1B,CAAU,EAAIC,GAGhC,CAAC,EACMyB,CACT,CAjBSN,EAAAe,EAAA,kBAmBT,SAASC,EAAmByC,EAAO,CACjC,GAAI,CACF,OAAIA,GACW,KAAK,MAAMA,EAAM,KAAK,EACvB,cAAgB,IAIhC,OAAS1D,EAAP,CACA,eAAQ,MAAM,iCAAkCA,CAAC,EAC1C,IACT,CACF,CAZSC,EAAAgB,EAAA,sBAcT,SAASmB,EAAYD,EAAQ,CAC3B,IAAIhB,EACEsD,EAAW,CAAC,EA8BlB,GA5BAC,EAAkBvC,CAAM,EAExBA,EAAO,QAASuB,GAAU,CACxB,IAAMiB,EAAOjB,EAAM,KACbxB,EAAQwB,EAAM,MACdhD,EAAOgD,EAAM,eACnB,GAAIiB,IAAS,OACXxD,EAAOe,UACEyC,IAAS,OACdF,EAAS/D,CAAI,EACf+D,EAAS/D,CAAI,EAAE,WAAW,KAAKwB,CAAK,EAEpCuC,EAAS/D,CAAI,EAAI,CACf,SAAU,KACV,WAAY,CAACwB,CAAK,CACpB,UAEOyC,IAAS,WACdF,EAAS/D,CAAI,EACf+D,EAAS/D,CAAI,EAAE,SAAWwB,EAE1BuC,EAAS/D,CAAI,EAAI,CAAE,SAAUwB,EAAO,WAAY,CAAC,CAAE,MAGrD,OAAM,IAAI,MAAM,aAAa,CAEjC,CAAC,EAEG,CAACf,EACH,MAAM,IAAI,MAAM,gDAAgD,EAGlE,MAAO,CAAE,KAAAA,EAAM,SAAAsD,CAAS,CAC1B,CArCSxE,EAAAmC,EAAA,eAuCT,SAASC,EACP,CAAE,YAAAuC,CAAY,EACdH,EACA5B,EACAC,EACA,CACA8B,EAAY,QAAQ3E,EAAA,SAA4B4E,EAAgBtF,EAAG,CACjE,IAAMmB,EAAOmE,EAAe,KACtBC,EAAkBL,EAAS/D,CAAI,EACrC,GAAIoE,EAAiB,CACnB,GAAI,CAACA,EAAgB,SACnB,MAAM,IAAI,MACR,mDAAmDpE,GACrD,EAEF,IAAM7B,EAAaiG,EAAgB,SACnCA,EAAgB,SAAW,KAC3BF,EAAYrF,CAAC,EAAIV,EAEjB,IAAMkG,EAASjC,EAAgB,SAASpC,CAAI,EACtCsE,EAAkC/E,EAAA,IAAM,CAC5CvB,GAASqG,GAAU,QAAQ,IAAI,gBAAgBrE,GAAM,EACrD,IAAMuE,EAAO,CAAC,EACRC,EAAaJ,EAAgB,WACnC,OAAAI,EAAW,QAAQjF,EAAA,SAAuByD,EAAOnE,EAAG,CAC9CwF,IACFrB,EAAQ,KAAK,MAAMA,CAAK,EACpBb,IACFa,EAAQb,EAAiBnC,EAAMgD,CAAK,IAGxCA,EAAM,QAASyB,GAAQ,CACrBF,EAAK,KAAKE,CAAG,CACf,CAAC,EACDD,EAAW3F,CAAC,EAAI,IAClB,EAXmB,gBAWlB,EACM0F,CACT,EAjBwC,mCAkBxCpG,EAAW,QAAUmG,EAEzB,EAlCoB,qBAkCnB,CACH,CAzCS/E,EAAAoC,EAAA,gBA2CT,SAAS+C,EAAc1B,EAAO,CAC5B,IAAMY,EAAMZ,EAAM,IAElB,GAAIY,IAAQ,OAAQ,CAClBZ,EAAM,KAAO,OACb,eACSY,EAAI,SAAS,GAAG,EAAG,CAC5B,IAAMC,EAAcD,EAAI,MAAM,GAAG,EACjC,GAAIC,EAAY,SAAW,GAAKA,EAAY,CAAC,IAAM,QAAS,CAC1Db,EAAM,KAAO,OACbA,EAAM,eAAiBa,EAAY,CAAC,EACpCb,EAAM,MAAQ,SAASa,EAAY,CAAC,EAAG,EAAE,EACzC,eACSA,EAAY,SAAW,GAAKA,EAAY,CAAC,IAAM,WAAY,CACpEb,EAAM,KAAO,WACbA,EAAM,eAAiBa,EAAY,CAAC,EACpC,QAIJ,cAAQ,MAAM,iBAAiBD,GAAK,EAC9B,IAAI,MAAM,0CAA0C,CAC5D,CAtBSrE,EAAAmF,EAAA,iBAwBT,SAASzB,EAAWD,EAAOb,EAAkBC,EAAiB,CAC5DsC,EAAc1B,CAAK,EAEnB,IAAM2B,EAAS3B,EAAM,OAAS,OACxBqB,EAASjC,EAAgB,SAASY,EAAM,cAAc,EAEtD2B,GAAUN,IACdrB,EAAM,MAAQ,KAAK,MAAMA,EAAM,KAAK,GAElCb,GAAoBwC,GAAU,CAACN,IACjCrB,EAAM,MAAQb,EAAiBa,EAAM,eAAgBA,EAAM,KAAK,EAEpE,CAZSzD,EAAA0D,EAAA,cAcT,SAAS5B,GAAkB,CAIzB,OAAO,KAAK,OAAO,EAAE,SAAS,EAAE,EAAE,UAAU,CAAC,CAC/C,CALS9B,EAAA8B,EAAA,mBAOT,SAAS2C,EAAkBvC,EAAQ,CAGjCA,EAAO,KAAK,SAAUmD,EAAGC,EAAG,CAC1B,OAAQD,EAAE,OAAS,IAAMC,EAAE,OAAS,EACtC,CAAC,CACH,CANStF,EAAAyE,EAAA,qBAQT,SAASvB,EAAgBH,EAAMwC,EAAO,CACpC,IAAMC,EAAgB,KAAK,MAAMzC,EAAK,OAASwC,CAAK,EAC9CtC,EAAY,CAAC,EACfwC,EACAC,EACJ,QAASpG,EAAI,EAAGA,EAAIiG,EAAOjG,GAAK,EAC9BmG,EAAS1C,EAAKyC,EAAgBlG,CAAC,EAC/BoG,EAAS3C,EAAKyC,GAAiBlG,EAAI,EAAE,EACjCA,IAAM,EAER2D,EAAU,KAAK,YAAY,WAAWyC,EAAQ,EAAI,CAAC,EAC1CpG,IAAMiG,EAAQ,EAEvBtC,EAAU,KAAK,YAAY,WAAWwC,CAAM,CAAC,EAG7CxC,EAAU,KAAK,YAAY,MAAMwC,EAAQC,EAAQ,GAAO,EAAI,CAAC,EAGjE,OAAOzC,CACT,CApBSjD,EAAAkD,EAAA,mBAsBT,SAASrC,EAAOsD,EAASwB,EAAWC,EAAS,CAC3C,OAAAzB,EAAQ,UAAapE,GAAM,CACzB,GAAI,CACF,OAAO4F,EAAU5F,CAAC,CACpB,OAASY,EAAP,CACAiF,EAAQjF,CAAK,CACf,CACF,EACAwD,EAAQ,QAAUyB,EACXzB,CACT,CAVSnE,EAAAa,EAAA,UAYL,SAAW,SACb,OAAO,4BAA8BnC",
  "names": ["DEBUG", "IncrementalIndexedDBAdapter", "options", "collection", "chunkId", "minId", "maxId", "idIndex", "firstDataPosition", "max", "min", "mid", "lastDataPosition", "i", "firstElement", "lastElement", "chunkData", "dbname", "getLokiCopy", "callback", "that", "finish", "e", "__name", "updatePrevVersionIds", "didOverwrite", "tx", "store", "performSave", "maxChunkIds", "incremental", "chunkInfo", "name", "versionId", "error", "getAllKeysThenSave", "idbReq", "target", "getMaxChunkIds", "lokiChunkVersionId", "idbStore", "loki", "collectionVersionIds", "savedSize", "prepareCollection", "dirtyChunks", "lokiId", "prepareChunk", "maxChunkId", "j", "persistedMaxChunkId", "k", "deletedChunkName", "randomVersionId", "metadataChunk", "serializedMetadata", "value", "chunks", "chunksToMap", "populateLoki", "idbVersionId", "onError", "onSuccess", "openRequest", "oldVersion", "db", "versionChangeEvent", "deserializeChunk", "lazyCollections", "getMegachunks", "keys", "megachunkCount", "keyRanges", "createKeyRanges", "allChunks", "megachunksReceived", "processMegachunk", "megachunkIndex", "keyRange", "megachunk", "chunk", "parseChunk", "megachunkWaves", "megachunksPerWave", "requestMegachunk", "index", "wave", "getAllChunks", "getAllKeys", "onDidGetKeys", "request", "allKeys", "key", "keySegments", "currentMax", "chunkMap", "sortChunksInPlace", "type", "collections", "collectionStub", "chunkCollection", "isLazy", "lokiDeserializeCollectionChunks", "data", "dataChunks", "doc", "classifyChunk", "isData", "a", "b", "count", "countPerRange", "minKey", "maxKey", "onsuccess", "onerror"]
}
